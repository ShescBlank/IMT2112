{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8807fd1d",
   "metadata": {},
   "source": [
    "# Multiprocessing tutorial 5\n",
    "\n",
    " - Author: Elwin van 't Wout\n",
    " - Affiliation: Pontificia Universidad Cat√≥lica de Chile\n",
    " - Course: IMT3870\n",
    " - Date: 12-8-2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f0f24",
   "metadata": {},
   "source": [
    "The library `joblib` provides functionality for parallel computing. In this notebook, let us look into shared variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc4770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0483e6cc",
   "metadata": {},
   "source": [
    "In the *multiprocessing* model, each process has its own data space with private variables. Hence, no variables can be shared between different tasks. The `joblib` library does allow for the use of global variables in each process, but they should not be changed by the different processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85184253",
   "metadata": {},
   "source": [
    "## Reading a global variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ee2978",
   "metadata": {},
   "source": [
    "Let us create a global variable with a specific value and a function that reads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2749eda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_GLOBAL_VAR = 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a4452e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_global_var():\n",
    "    return MY_GLOBAL_VAR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fae770",
   "metadata": {},
   "source": [
    "Let us perform this task with multiple processes. That is, each process returns the global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [delayed(return_global_var)() for i in range(n_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca1a1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_workers, batch_size=1, verbose=10, backend='loky') as parallel_pool:\n",
    "    parallel_results = parallel_pool(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16550bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c515f3e",
   "metadata": {},
   "source": [
    "Even though each process has an independent dataspace, the variables created earlier in the notebook can also be used. However, this does not mean that the variable is actually shared in the sense that both processes can access the same memory where the variable is stored. The `joblib` library made a copy of the global variable in each process. Hence, it cannot be changed by the individual processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e1bd9a",
   "metadata": {},
   "source": [
    "## Writing into a global variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3555c7f",
   "metadata": {},
   "source": [
    "Let us try to overwrite a global variable with different values in each process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67926769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_global_var(n):\n",
    "    MY_GLOBAL_VAR = n\n",
    "    return MY_GLOBAL_VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c81c7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [delayed(change_global_var)(i) for i in range(n_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d546c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_workers, batch_size=1, verbose=10, backend='loky') as parallel_pool:\n",
    "    parallel_results = parallel_pool(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d047817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_results)\n",
    "print(MY_GLOBAL_VAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a717543",
   "metadata": {},
   "source": [
    "The above results shows that in each process, a local variable named `MY_GLOBAL_VAR` was created and returned to the main process. The global variable with the same name `MY_GLOBAL_VAR` was left unchanged. Notice that this is the expected behaviour of any Python function, not just for `joblib`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209e2d79",
   "metadata": {},
   "source": [
    "## Changing a global variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882fb8e4",
   "metadata": {},
   "source": [
    "Let us try to add a value to the global variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244968a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_global_var(n):\n",
    "    MY_GLOBAL_VAR += n\n",
    "    return MY_GLOBAL_VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607919c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = [delayed(add_to_global_var)(i) for i in range(n_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb30038",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_workers, batch_size=1, verbose=10, backend='loky') as parallel_pool:\n",
    "    parallel_results = parallel_pool(tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f48a16",
   "metadata": {},
   "source": [
    "The `joblib` library throws an error. Here, the function tries to read and then write into the same variable `my_global_var`. The previous examples showed that either reading or writing is possible, but adding to a global variable fails. The reason is that Python cannot detect if the variable is a global or local variable since we try to both read and write the variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7fcbfd",
   "metadata": {},
   "source": [
    "## Reading databases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52092b7b",
   "metadata": {},
   "source": [
    "In data science, it is common to have a dataset that needs to be used by all processes. However, each process has its own data space. There are different ways to handle this situation. The easiest approach is to handle the data set as a global variable. This is sufficient if the processes only need to read the dataset but not adapt it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1493f80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0467d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_global_df = pd.DataFrame(data=np.arange(100), columns=[\"my_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41152db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_data(start, end):\n",
    "    my_local_data = my_global_df[\"my_data\"][start:end]\n",
    "    return np.sum(my_local_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5d2231",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = my_global_df.shape[0] // n_workers\n",
    "tasks = [delayed(sum_data)(i*chunk_size, (i+1)*chunk_size) for i in range(n_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_workers, batch_size=1, verbose=10, backend='loky') as parallel_pool:\n",
    "    parallel_results = parallel_pool(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2650d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd3a23",
   "metadata": {},
   "source": [
    "The first process indeed summed all elements in the first half of the database, and the second process summed the second half. Although this works, both processes have a copy of the entire database, which is inefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e97c44a",
   "metadata": {},
   "source": [
    "One way of distributing a database over different processes is by reading the necessary parts of the database in each process. For example, one worker reads the first half and the other worker the second half of a database from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2b718c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_global_df.to_excel(\"my_database.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f4053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_sum_data(start, end):\n",
    "    my_local_df = pd.read_excel(\"my_database.xlsx\", header=0, skiprows=range(1,start+1), nrows=end-start)\n",
    "    return my_local_df.shape, np.sum(my_local_df[\"my_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edad3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = my_global_df.shape[0] // n_workers\n",
    "tasks = [delayed(read_and_sum_data)(i*chunk_size, (i+1)*chunk_size) for i in range(n_workers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Parallel(n_jobs=n_workers, batch_size=1, verbose=10, backend='loky') as parallel_pool:\n",
    "    parallel_results = parallel_pool(tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f3257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parallel_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e813b4a",
   "metadata": {},
   "source": [
    "The result shows that the local data frames are half the size of the Excel file. Furthermore, the summations are correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
